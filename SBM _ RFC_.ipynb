{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4a16ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba6b982c",
   "metadata": {},
   "source": [
    "# SBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "e9d682a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import all the necessary libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy.stats import skew, kurtosis\n",
    "from scipy import integrate\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ec81a707",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fft(signal, window_size=8192, overlap_percentage=75, sample_rate=50000):\n",
    "    # generate a Hanning window of the window size\n",
    "    hanning_window = np.hanning(window_size)\n",
    "    overlap_samples = int((window_size * overlap_percentage) / 100)\n",
    "\n",
    "    # calculate the number of frames\n",
    "    num_frames = (len(signal) - window_size) // overlap_samples + 1\n",
    "\n",
    "    # initialize arrays to store FFT results and frequencies\n",
    "    fft_results = np.zeros((num_frames, window_size//2+1), dtype=complex)\n",
    "    frequencies = np.fft.rfftfreq(window_size, 1/sample_rate)\n",
    "\n",
    "    # apply Hanning window, compute FFT, and store the results\n",
    "    for i in range(num_frames):\n",
    "        start = i * overlap_samples\n",
    "        end = start + window_size\n",
    "        frame = signal[start:end] * hanning_window\n",
    "        fft_results[i, :] = np.fft.rfft(frame, window_size)\n",
    "\n",
    "    # calculate the magnitude spectrum\n",
    "    magnitude_spectrum = (np.abs(fft_results))/100\n",
    "\n",
    "    # calculate the average magnitude spectrum\n",
    "    avg_magnitude_spectrum = np.mean(magnitude_spectrum, axis=0)\n",
    "    \n",
    "    return avg_magnitude_spectrum, frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "a3aec3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "database_path = 'mafaulda' # load the database path\n",
    "N = 250000 # the number of points in the time series in each column\n",
    "sample_rate = 50000 # sampling rate\n",
    "Nfft = 8192 # no of FFT points\n",
    "window_size = 8192\n",
    "overlap_percentage = 75\n",
    "cutoff_frequency_high = 10\n",
    "cutoff_frequency_low = 10000\n",
    "\n",
    "# function that takes a csv file path and gives features corresponding to that file\n",
    "def calculate_features(file_path):\n",
    "    df = pd.read_csv(file_path, header=None) # load the dataframe\n",
    "    df.columns = [str(i) for i in range(len(df.columns))] # name the columns\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column]/(np.std(df[column].values))\n",
    "        # design filters\n",
    "        b_high, a_high = scipy.signal.butter(N=4, Wn=cutoff_frequency_high, btype='high', fs=sample_rate)\n",
    "        b_low, a_low = scipy.signal.butter(N=4, Wn=cutoff_frequency_low, btype='low', fs=sample_rate)\n",
    "        # apply the filter to your signal\n",
    "        df[column] = scipy.signal.filtfilt(b_high, a_high, df[column].values)\n",
    "        df[column] = scipy.signal.filtfilt(b_low, a_low, df[column].values)\n",
    "    \n",
    "    # calculate a rotation frequency\n",
    "    tachometer_signal = df['0'].values # load the tachometer signal\n",
    "    tacho_fft, freqs = get_fft(tachometer_signal, window_size, overlap_percentage, sample_rate)\n",
    "    \n",
    "    freq_max = [] # list of the top four frequencies which give peak\n",
    "    freq_max_idx = [] # list of the top four frequency indices which give peak\n",
    "    fft_dummy = tacho_fft # copy the fft\n",
    "\n",
    "    for i in range(4):\n",
    "        max_idx = np.argmax(fft_dummy) # calculate the index corresponding to the peak\n",
    "        f = max_idx * (sample_rate/Nfft) # calculate the corresponding frequency\n",
    "        freq_max.append(f) # append the frequency to the list\n",
    "        freq_max_idx.append(max_idx) # append the index to the list\n",
    "        for j in range(-3, 4):\n",
    "            fft_dummy[max_idx + j] = 0 # make the fft value zero at indices from max_idx-3 to max_idx+3 and iterate the process\n",
    "    \n",
    "    fr = np.min(freq_max) # rotating freq will be the minimum of all four frequencies\n",
    "    fr_index = freq_max_idx[np.argmin(freq_max)] # extract the index corresponding to the rotating frequency\n",
    "        \n",
    "    # calculate spectrum features at fr, 2fr and 3fr for all the remaining signals\n",
    "    acc_spectrum_features = [] # create an empty list for acceleration spectrum features\n",
    "    vel_spectrum_features = [] # create an empty list for velocity spectrum features\n",
    "    for i in range(1, 4):\n",
    "        freq_index = i * fr_index # for multiples of fr\n",
    "        freqs_adjacent_idx = [freq_index - i for i in range(5, 0, -1)] + [freq_index] + [freq_index + i for i in range(1, 6)]\n",
    "\n",
    "        for column in df.columns[1:]:\n",
    "            if (int(column) != 0 and int(column) != 7):\n",
    "                vel_signal = integrate.cumtrapz(df[column], initial=0, dx=1/sample_rate)\n",
    "                vel_fft_signal, vel_freqs = get_fft(vel_signal, window_size, overlap_percentage, sample_rate)\n",
    "                vel_spectrum_feature = np.mean([vel_fft_signal[idx] for idx in freqs_adjacent_idx]) \n",
    "                vel_spectrum_features.append(vel_spectrum_feature)\n",
    "            acc_signal = df[column]\n",
    "            acc_fft_signal, acc_freqs = get_fft(acc_signal, window_size, overlap_percentage, sample_rate)\n",
    "            acc_spectrum_feature = np.mean([acc_fft_signal[idx] for idx in freqs_adjacent_idx]) \n",
    "            acc_spectrum_features.append(acc_spectrum_feature)\n",
    "    \n",
    "    # calculate statistical features for all the signals\n",
    "    statistical_features_acc = [] # create an empty list for statistical features of acceleration\n",
    "    statistical_features_vel = [] # create an empty list for statistical features of velocity\n",
    "    for column in df.columns:\n",
    "        if (int(column) != 0 and int(column) != 7):\n",
    "            vel = integrate.cumtrapz(df[column], initial=0, dx=1/sample_rate)\n",
    "            vel_mean_val = vel.mean()\n",
    "            vel_median_val = np.median(vel)\n",
    "            vel_std_val = vel.std()\n",
    "            vel_rms_val = np.sqrt(np.mean(vel**2))\n",
    "            statistical_features_vel.extend([vel_mean_val, vel_median_val, vel_std_val, vel_rms_val])\n",
    "        acc_mean_val = df[column].mean() # calculate mean value\n",
    "        acc_median_val = df[column].median() # calculate median value\n",
    "        acc_std_val = df[column].std() # calculate standard deviation value\n",
    "        acc_rms_val = np.sqrt(np.mean(df[column]**2)) # calculate rms value\n",
    "#         skew_val = skew(df[column]) # calculate skewness value\n",
    "#         kurtosis_val = kurtosis(df[column]) # calculate kurtosis value\n",
    "        statistical_features_acc.extend([acc_mean_val, acc_median_val, acc_std_val, acc_rms_val])\n",
    "        \n",
    "    # combine all the features to form a feature vector for the file\n",
    "    all_features = [fr] + acc_spectrum_features + vel_spectrum_features + statistical_features_acc + statistical_features_vel\n",
    "    return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "0f3e4703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time needed to extract normal features:  1.758551788330078 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # start time for extracting features\n",
    "\n",
    "# normal class features\n",
    "normal_features_list = [] # create an empty list for normal features\n",
    "normal_class_path = os.path.join(database_path, 'normal')\n",
    "\n",
    "if os.path.isdir(normal_class_path): # check if it's a directory\n",
    "    for file_name in os.listdir(normal_class_path): # iterate through each CSV file in the folder\n",
    "        file_path = os.path.join(normal_class_path, file_name)\n",
    "        \n",
    "        # check if it's a file\n",
    "        if os.path.isfile(file_path) and file_name.endswith(\".csv\"): \n",
    "            features = calculate_features(file_path)\n",
    "            normal_features_list.append(features)\n",
    "\n",
    "# create a dataframe from the list of all features\n",
    "result_df = pd.DataFrame(normal_features_list)\n",
    "\n",
    "# save the result dataframe to a new CSV file\n",
    "result_df.to_csv(\"normal_features.csv\", index=False)\n",
    "\n",
    "end_time = time.time() # end time for extracting features\n",
    "\n",
    "print('Time needed to extract normal features: ', (end_time - start_time)/60, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "80636b47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time needed to extract imbalance features:  11.730530261993408 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # start time for extracting features\n",
    "\n",
    "# imbalance class features\n",
    "imbalance_features_list = [] # create an empty list for imbalance features\n",
    "imbalance_class_path = os.path.join(database_path, 'imbalance')\n",
    "\n",
    "for folder_name in os.listdir(imbalance_class_path): # go to each of the folders in it\n",
    "    folder_path = os.path.join(imbalance_class_path, folder_name)\n",
    "    \n",
    "    # check if it's a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # iterate through each CSV file in the folder\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # check if it's a file\n",
    "            if os.path.isfile(file_path) and file_name.endswith(\".csv\"):\n",
    "                features = calculate_features(file_path)\n",
    "                imbalance_features_list.append(features)\n",
    "\n",
    "# create a dataframe from the list of all features\n",
    "result_df = pd.DataFrame(imbalance_features_list)\n",
    "\n",
    "# save the result dataframe to a new CSV file\n",
    "result_df.to_csv(\"imbalance_features.csv\", index=False)\n",
    "\n",
    "end_time = time.time() # end time for extracting features\n",
    "\n",
    "print('Time needed to extract imbalance features: ', (end_time - start_time)/60, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "dbb0232b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time needed to extract horizontal features:  6.952353421847025 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # start time for extracting features\n",
    "\n",
    "# horizontal parallel misalignment class features\n",
    "horizontal_features_list = [] # create an empty list for horizontal parallel misalignment features\n",
    "horizontal_class_path = os.path.join(database_path, 'horizontal-misalignment')\n",
    "\n",
    "for folder_name in os.listdir(horizontal_class_path): # go to each of the folders in it\n",
    "    folder_path = os.path.join(horizontal_class_path, folder_name)\n",
    "    \n",
    "    # check if it's a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # iterate through each CSV file in the folder\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # check if it's a file\n",
    "            if os.path.isfile(file_path) and file_name.endswith(\".csv\"):\n",
    "                features = calculate_features(file_path)\n",
    "                horizontal_features_list.append(features)\n",
    "\n",
    "# create a dataframe from the list of all features\n",
    "result_df = pd.DataFrame(horizontal_features_list)\n",
    "\n",
    "# save the result dataframe to a new CSV file\n",
    "result_df.to_csv(\"horizontal_features.csv\", index=False)\n",
    "\n",
    "end_time = time.time() # end time for extracting features\n",
    "\n",
    "print('Time needed to extract horizontal features: ', (end_time - start_time)/60, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "f43a27ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time needed to extract vertical features:  10.553336644172669 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # start time for extracting features\n",
    "\n",
    "# vertical parallel misalignment class features\n",
    "vertical_features_list = [] # create an empty list for vertical parallel misalignment features\n",
    "vertical_class_path = os.path.join(database_path, 'vertical-misalignment')\n",
    "\n",
    "for folder_name in os.listdir(vertical_class_path): # go to each of the folders in it\n",
    "    folder_path = os.path.join(vertical_class_path, folder_name)\n",
    "    \n",
    "    # check if it's a directory\n",
    "    if os.path.isdir(folder_path):\n",
    "        # iterate through each CSV file in the folder\n",
    "        for file_name in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "\n",
    "            # check if it's a file\n",
    "            if os.path.isfile(file_path) and file_name.endswith(\".csv\"):\n",
    "                features = calculate_features(file_path)\n",
    "                vertical_features_list.append(features)\n",
    "\n",
    "# create a dataframe from the list of all features\n",
    "result_df = pd.DataFrame(vertical_features_list)\n",
    "\n",
    "# save the result dataframe to a new CSV file\n",
    "result_df.to_csv(\"vertical_features.csv\", index=False)\n",
    "\n",
    "end_time = time.time() # end time for extracting features\n",
    "\n",
    "print('Time needed to extract vertical features: ', (end_time - start_time)/60, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "2689f602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time needed to extract underhang features:  19.652845307191214 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # start time for extracting features\n",
    "\n",
    "# underhang class features\n",
    "underhang_features_list = [] # create an empty list for underhang features\n",
    "underhang_class_path = os.path.join(database_path, 'underhang')\n",
    "\n",
    "for folder_name in os.listdir(underhang_class_path): # go to each of the folders in it\n",
    "    folder_path = os.path.join(underhang_class_path, folder_name)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        for subfolder_name in os.listdir(folder_path): # go to each of the subfolders in it\n",
    "            subfolder_path = os.path.join(folder_path, subfolder_name)\n",
    "            \n",
    "            # check if it's a directory\n",
    "            if os.path.isdir(subfolder_path):\n",
    "                # iterate through each CSV file in the subfolder\n",
    "                for file_name in os.listdir(subfolder_path):\n",
    "                    file_path = os.path.join(subfolder_path, file_name)\n",
    "                    \n",
    "                    # check if it's a file\n",
    "                    if os.path.isfile(file_path) and file_name.endswith(\".csv\"):\n",
    "                        features = calculate_features(file_path)\n",
    "                        underhang_features_list.append(features)\n",
    "        \n",
    "        \n",
    "        \n",
    "# create a dataframe from the list of all features\n",
    "result_df = pd.DataFrame(underhang_features_list)\n",
    "\n",
    "# save the result dataframe to a new CSV file\n",
    "result_df.to_csv(\"underhang_features.csv\", index=False)\n",
    "\n",
    "end_time = time.time() # end time for extracting features\n",
    "\n",
    "print('Time needed to extract underhang features: ', (end_time - start_time)/60, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "0c36cc74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time needed to extract overhang features:  18.44502481619517 min\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time() # start time for extracting features\n",
    "\n",
    "# overhang class features\n",
    "overhang_features_list = [] # create an empty list for overhang features\n",
    "overhang_class_path = os.path.join(database_path, 'overhang')\n",
    "\n",
    "for folder_name in os.listdir(overhang_class_path): # go to each of the folders in it\n",
    "    folder_path = os.path.join(overhang_class_path, folder_name)\n",
    "    \n",
    "    if os.path.isdir(folder_path):\n",
    "        for subfolder_name in os.listdir(folder_path): # go to each of the subfolders in it\n",
    "            subfolder_path = os.path.join(folder_path, subfolder_name)\n",
    "            \n",
    "            # check if it's a directory\n",
    "            if os.path.isdir(subfolder_path):\n",
    "                # iterate through each CSV file in the subfolder\n",
    "                for file_name in os.listdir(subfolder_path):\n",
    "                    file_path = os.path.join(subfolder_path, file_name)\n",
    "                    \n",
    "                    # check if it's a file\n",
    "                    if os.path.isfile(file_path) and file_name.endswith(\".csv\"):\n",
    "                        features = calculate_features(file_path)\n",
    "                        overhang_features_list.append(features)\n",
    "        \n",
    "        \n",
    "        \n",
    "# create a dataframe from the list of all features\n",
    "result_df = pd.DataFrame(overhang_features_list)\n",
    "\n",
    "# save the result dataframe to a new CSV file\n",
    "result_df.to_csv(\"overhang_features.csv\", index=False)\n",
    "\n",
    "end_time = time.time() # end time for extracting features\n",
    "\n",
    "print('Time needed to extract overhang features: ', (end_time - start_time)/60, 'min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "93eba8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate similarity using cauchy kernel similarity\n",
    "def calculate_similarity(xi, xj, gamma=0.005):\n",
    "    dij = np.linalg.norm(xi - xj, ord=2) # calculate the distance\n",
    "    similarity = 1 / np.sqrt(1 + (gamma**2) * (dij**2)) # based on the distance, calculate the similarity\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "1d873c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function which takes features file to create memory matrix\n",
    "def memory_matrix(features_file, t=11):\n",
    "    df = pd.read_csv(features_file) # read the dataframe\n",
    "    X = df.values # extract 2D array from the dataframe\n",
    "    \n",
    "    # first set of vectors for the memory matrix\n",
    "    extrema_indices = [] # create an empty list to accomodate the indices corresponding to extremas\n",
    "    for m in range(X.shape[1]):\n",
    "        min_index = np.argmin(X[:, m]) # extract index corresponding to the minimum\n",
    "        max_index = np.argmax(X[:, m]) # extract index corresponding to the maximum\n",
    "        extrema_indices.extend([min_index, max_index]) # add them to the list\n",
    "\n",
    "    unique_extrema_indices = np.unique(extrema_indices) # just keep unique indices\n",
    "    representatives_from_extrema = X[unique_extrema_indices] # extract the required first set of vectors\n",
    "\n",
    "    # second set of vectors for the memory matrix\n",
    "    remaining_indices = [i for i in range(X.shape[0]) if i not in unique_extrema_indices] # array of remaining indices\n",
    "    sorted_remaining_indices = sorted(remaining_indices, key=lambda i: np.linalg.norm(X[i], ord=2), reverse=True) # sort them in the decreasing order of the l2 norm values\n",
    "    decimated_remaining_indices = sorted_remaining_indices[::t] # decimate them by a factor of t\n",
    "    representatives_from_decimated = X[decimated_remaining_indices] # extract the required second set of vectors\n",
    "\n",
    "    # Combine representative samples from extrema and decimated remaining samples\n",
    "    model_matrix = np.concatenate([representatives_from_extrema, representatives_from_decimated])\n",
    "    \n",
    "    testing_matrix = np.delete(X, np.concatenate([unique_extrema_indices, decimated_remaining_indices]), axis=0)\n",
    "\n",
    "    return model_matrix, testing_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "6c7bb293",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normal matrix dimension:  (39, 96) (10, 96)\n",
      "imbalance matrix dimensions:  (103, 96) (230, 96)\n",
      "horizontal matrix dimensions:  (82, 96) (115, 96)\n",
      "vertical matrix dimensions:  (94, 96) (207, 96)\n",
      "underhang matrix dimensions:  (128, 96) (430, 96)\n",
      "overhang matrix dimensions:  (126, 96) (387, 96)\n"
     ]
    }
   ],
   "source": [
    "# memory matrices for all the classes\n",
    "\n",
    "# memory matrix for normal class\n",
    "normal_matrix, normal_test_matrix = memory_matrix(features_file='normal_features.csv')\n",
    "print(\"normal matrix dimension: \", normal_matrix.shape, normal_test_matrix.shape)\n",
    "\n",
    "# memory matrix for imbalance class\n",
    "imbalance_matrix, imbalance_test_matrix = memory_matrix(features_file='imbalance_features.csv')\n",
    "print(\"imbalance matrix dimensions: \", imbalance_matrix.shape, imbalance_test_matrix.shape)\n",
    "\n",
    "# memory matrix for horizontal misalignment class\n",
    "horizontal_matrix, horizontal_test_matrix = memory_matrix(features_file='horizontal_features.csv')\n",
    "print(\"horizontal matrix dimensions: \", horizontal_matrix.shape, horizontal_test_matrix.shape)\n",
    "\n",
    "# memory matrix for vertical misalignment class\n",
    "vertical_matrix, vertical_test_matrix = memory_matrix(features_file='vertical_features.csv')\n",
    "print(\"vertical matrix dimensions: \", vertical_matrix.shape, vertical_test_matrix.shape)\n",
    "\n",
    "# memory matrix for underhang class\n",
    "underhang_matrix, underhang_test_matrix = memory_matrix(features_file='underhang_features.csv')\n",
    "print(\"underhang matrix dimensions: \", underhang_matrix.shape, underhang_test_matrix.shape)\n",
    "\n",
    "# memory matrix for overhang class\n",
    "overhang_matrix, overhang_test_matrix = memory_matrix(features_file='overhang_features.csv')\n",
    "print(\"overhang matrix dimensions: \", overhang_matrix.shape, overhang_test_matrix.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b27d94b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate the similarity of any incoming vector with respect to a memory matrix\n",
    "def similarity_using_estimate(inc_vec, mem_mat):\n",
    "    # assuming memory matrix to be of the size L x M\n",
    "    L, M = mem_mat.shape\n",
    "    \n",
    "    # initialize a matrix G\n",
    "    G = np.zeros((L, L))\n",
    "    \n",
    "    # calculate elements of G using the similarity function\n",
    "    for i in range(L):\n",
    "        for j in range(L):\n",
    "            G[i, j] = calculate_similarity(mem_mat[i, :], mem_mat.T[:, j])\n",
    "    \n",
    "    # calculate inverse of the G matrix\n",
    "    inv_G = np.linalg.inv(G)\n",
    "    \n",
    "    # calculate the vector a_n\n",
    "    a_n = [calculate_similarity(mem_mat[i, :], inc_vec) for i in range(L)]\n",
    "    \n",
    "    # calculate the weight vector w_n\n",
    "    w_n = np.dot(inv_G, a_n)\n",
    "    \n",
    "    # calculate the normalized weight vector\n",
    "    normalized_w_n = w_n / np.linalg.norm(w_n, ord=1)\n",
    "    \n",
    "    # calculate the estimate\n",
    "    estimate = np.dot(mem_mat.T, normalized_w_n)\n",
    "    \n",
    "    similarity = calculate_similarity(inc_vec, estimate)\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "e2e22df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to estimate the class of an incoming vector based on the similarity measure\n",
    "def estimate_class(inc_vec):\n",
    "    similarity_list = []\n",
    "    \n",
    "    # similarity with the normal class\n",
    "    normal_similarity = similarity_using_estimate(inc_vec, normal_matrix)\n",
    "    similarity_list.append(normal_similarity)\n",
    "    \n",
    "    # similarity with the imbalance class\n",
    "    imbalance_similarity = similarity_using_estimate(inc_vec, imbalance_matrix)\n",
    "    similarity_list.append(imbalance_similarity)\n",
    "    \n",
    "    # similarity with the horizontal misalignment class\n",
    "    horizontal_similarity = similarity_using_estimate(inc_vec, horizontal_matrix)\n",
    "    similarity_list.append(horizontal_similarity)\n",
    "    \n",
    "    # similarity with the vertical misalignment class\n",
    "    vertical_similarity = similarity_using_estimate(inc_vec, vertical_matrix)\n",
    "    similarity_list.append(vertical_similarity)\n",
    "    \n",
    "    # similarity with the underhang class\n",
    "    underhang_similarity = similarity_using_estimate(inc_vec, underhang_matrix)\n",
    "    similarity_list.append(underhang_similarity)\n",
    "    \n",
    "    # similarity with the overhang class\n",
    "    overhang_similarity = similarity_using_estimate(inc_vec, overhang_matrix)\n",
    "    similarity_list.append(overhang_similarity)\n",
    "    \n",
    "    # estimate the class index based on maximum similarity\n",
    "    class_estimate_index = np.argmax(similarity_list)\n",
    "    \n",
    "    # mapping showing the class label names\n",
    "    label_mapping = {0: 'Normal', 1: 'Imbalance Fault', 2: 'Horizontal Parallel Misalignment Fault', \n",
    "                 3: 'Vertical Parallel Misalignment Fault', 4: 'Underhang Bearing Fault', 5: 'Overhang Bearing Fault'}\n",
    "    \n",
    "    class_estimate = label_mapping[class_estimate_index]\n",
    "    \n",
    "    return class_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "8217f981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated class: Underhang Bearing Fault | Time taken for prediction: 0.0 microsec\n"
     ]
    }
   ],
   "source": [
    "# testing of individual vectors\n",
    "start_time = time.time()\n",
    "X = underhang_test_matrix\n",
    "vec = X[5]\n",
    "end_time = time.time()\n",
    "print(\"Estimated class:\", estimate_class(vec), \"| Time taken for prediction:\", (end_time - start_time)*1e6, \"microsec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f3b6be06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0 | Time taken: 0.24444880882898967 min\n"
     ]
    }
   ],
   "source": [
    "# testing of each class\n",
    "start_time = time.time()\n",
    "X = normal_test_matrix\n",
    "accumulate = 0\n",
    "num_vec = X.shape[0]\n",
    "for i in range(num_vec):\n",
    "    class_type = estimate_class(X[i])\n",
    "    if class_type == 'Normal':\n",
    "        accumulate += 1\n",
    "\n",
    "accuracy = (accumulate/num_vec)*100\n",
    "end_time = time.time()\n",
    "print(\"Accuracy:\", accuracy, \"| Time taken:\", (end_time - start_time)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "4d4589a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.69565217391305 | Time taken: 5.516671466827392 min\n"
     ]
    }
   ],
   "source": [
    "# testing of each class\n",
    "start_time = time.time()\n",
    "X = imbalance_test_matrix\n",
    "accumulate = 0\n",
    "num_vec = X.shape[0]\n",
    "for i in range(num_vec):\n",
    "    class_type = estimate_class(X[i])\n",
    "    if class_type == 'Imbalance Fault':\n",
    "        accumulate += 1\n",
    "\n",
    "accuracy = (accumulate/num_vec)*100\n",
    "end_time = time.time()\n",
    "print(\"Accuracy:\", accuracy, \"| Time taken:\", (end_time - start_time)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "4fbe9618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.52173913043478 | Time taken: 2.692330479621887 min\n"
     ]
    }
   ],
   "source": [
    "# testing of each class\n",
    "start_time = time.time()\n",
    "X = horizontal_test_matrix\n",
    "accumulate = 0\n",
    "num_vec = X.shape[0]\n",
    "for i in range(num_vec):\n",
    "    class_type = estimate_class(X[i])\n",
    "    if class_type == 'Horizontal Parallel Misalignment Fault':\n",
    "        accumulate += 1\n",
    "\n",
    "accuracy = (accumulate/num_vec)*100\n",
    "end_time = time.time()\n",
    "print(\"Accuracy:\", accuracy, \"| Time taken:\", (end_time - start_time)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "f8acdf4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.03381642512076 | Time taken: 4.877285444736481 min\n"
     ]
    }
   ],
   "source": [
    "# testing of each class\n",
    "start_time = time.time()\n",
    "X = vertical_test_matrix\n",
    "accumulate = 0\n",
    "num_vec = X.shape[0]\n",
    "for i in range(num_vec):\n",
    "    class_type = estimate_class(X[i])\n",
    "    if class_type == 'Vertical Parallel Misalignment Fault':\n",
    "        accumulate += 1\n",
    "\n",
    "accuracy = (accumulate/num_vec)*100\n",
    "end_time = time.time()\n",
    "print(\"Accuracy:\", accuracy, \"| Time taken:\", (end_time - start_time)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "8a47806d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.72093023255815 | Time taken: 10.051030913988749 min\n"
     ]
    }
   ],
   "source": [
    "# testing of each class\n",
    "start_time = time.time()\n",
    "X = underhang_test_matrix\n",
    "accumulate = 0\n",
    "num_vec = X.shape[0]\n",
    "for i in range(num_vec):\n",
    "    class_type = estimate_class(X[i])\n",
    "    if class_type == 'Underhang Bearing Fault':\n",
    "        accumulate += 1\n",
    "\n",
    "accuracy = (accumulate/num_vec)*100\n",
    "end_time = time.time()\n",
    "print(\"Accuracy:\", accuracy, \"| Time taken:\", (end_time - start_time)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "04af74c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.44961240310077 | Time taken: 9.25152721007665 min\n"
     ]
    }
   ],
   "source": [
    "# testing of each class\n",
    "start_time = time.time()\n",
    "X = overhang_test_matrix\n",
    "accumulate = 0\n",
    "num_vec = X.shape[0]\n",
    "for i in range(num_vec):\n",
    "    class_type = estimate_class(X[i])\n",
    "    if class_type == 'Overhang Bearing Fault':\n",
    "        accumulate += 1\n",
    "\n",
    "accuracy = (accumulate/num_vec)*100\n",
    "end_time = time.time()\n",
    "print(\"Accuracy:\", accuracy, \"| Time taken:\", (end_time - start_time)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "12341fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.95431472081218 | Time taken: 13.296646293004354 min\n"
     ]
    }
   ],
   "source": [
    "# testing of overall SBM model\n",
    "start_time = time.time()\n",
    "test_matrices = [normal_test_matrix, imbalance_test_matrix, horizontal_test_matrix, vertical_test_matrix, underhang_test_matrix, overhang_test_matrix]\n",
    "accumulate = 0\n",
    "num_vec_total = 0\n",
    "for i in range(len(test_matrices)):\n",
    "    X = test_matrices[i]\n",
    "    num_vec_total += X.shape[0]\n",
    "\n",
    "# mapping showing the class label names\n",
    "label_mapping = {0: 'Normal', 1: 'Imbalance Fault', 2: 'Horizontal Parallel Misalignment Fault', \n",
    "                 3: 'Vertical Parallel Misalignment Fault', 4: 'Underhang Bearing Fault', 5: 'Overhang Bearing Fault'}\n",
    "\n",
    "for idx, test_matrix in enumerate(test_matrices):\n",
    "    X = test_matrix\n",
    "    num_vec_in_this = X.shape[0]\n",
    "    for i in range(num_vec_in_this):\n",
    "        class_type = estimate_class(X[i])\n",
    "        if class_type == label_mapping[idx]:\n",
    "            accumulate += 1\n",
    "\n",
    "accuracy = (accumulate/num_vec_total)*100\n",
    "end_time = time.time()\n",
    "print(\"Accuracy:\", accuracy, \"| Time taken:\", (end_time - start_time)/60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6c3adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3779a42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "56f662e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to estimate similarity measures and the class of an incoming vector based on the similarity measure\n",
    "def estimate_similarity_class(inc_vec):\n",
    "    similarity_list = []\n",
    "    \n",
    "    # similarity with the normal class\n",
    "    normal_similarity = similarity_using_estimate(inc_vec, normal_matrix)\n",
    "    similarity_list.append(normal_similarity)\n",
    "    \n",
    "    # similarity with the imbalance class\n",
    "    imbalance_similarity = similarity_using_estimate(inc_vec, imbalance_matrix)\n",
    "    similarity_list.append(imbalance_similarity)\n",
    "    \n",
    "    # similarity with the horizontal misalignment class\n",
    "    horizontal_similarity = similarity_using_estimate(inc_vec, horizontal_matrix)\n",
    "    similarity_list.append(horizontal_similarity)\n",
    "    \n",
    "    # similarity with the vertical misalignment class\n",
    "    vertical_similarity = similarity_using_estimate(inc_vec, vertical_matrix)\n",
    "    similarity_list.append(vertical_similarity)\n",
    "    \n",
    "    # similarity with the underhang class\n",
    "    underhang_similarity = similarity_using_estimate(inc_vec, underhang_matrix)\n",
    "    similarity_list.append(underhang_similarity)\n",
    "    \n",
    "    # similarity with the overhang class\n",
    "    overhang_similarity = similarity_using_estimate(inc_vec, overhang_matrix)\n",
    "    similarity_list.append(overhang_similarity)\n",
    "    \n",
    "    # estimate the class index based on maximum similarity\n",
    "    class_estimate_index = np.argmax(similarity_list)\n",
    "    \n",
    "    # mapping showing the class label names\n",
    "    label_mapping = {0: 'Normal', 1: 'Imbalance Fault', 2: 'Horizontal Parallel Misalignment Fault', \n",
    "                 3: 'Vertical Parallel Misalignment Fault', 4: 'Underhang Bearing Fault', 5: 'Overhang Bearing Fault'}\n",
    "    \n",
    "    class_estimate = label_mapping[class_estimate_index]\n",
    "    \n",
    "    return class_estimate, similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "47000da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9954824342417192, 0.9954065795336863, 0.9954313998164173, 0.9954220672463203, 0.9961786491793477, 0.9957563349477487]\n",
      "Top most probable faults: ['Underhang Bearing Fault', 'Overhang Bearing Fault', 'Horizontal Parallel Misalignment Fault']\n",
      "\n",
      "Time taken for prediction: 572.4105834960938 msec\n"
     ]
    }
   ],
   "source": [
    "# predicting the fault\n",
    "\n",
    "# mapping that gives class name based on the index\n",
    "label_mapping = {0: 'Normal', 1: 'Imbalance Fault', 2: 'Horizontal Parallel Misalignment Fault', \n",
    "                 3: 'Vertical Parallel Misalignment Fault', 4: 'Underhang Bearing Fault', 5: 'Overhang Bearing Fault'}\n",
    "\n",
    "start_time = time.time()\n",
    "X = underhang_test_matrix # load any matrix\n",
    "vec = X[4] # extract any vector\n",
    "top3_faults = [] # list to store top 3 fault types\n",
    "\n",
    "estimated_class, similarity_measures = estimate_similarity_class(vec) # calculate the estimated class and similarities\n",
    "print(similarity_measures)\n",
    "\n",
    "if(estimated_class == \"Normal\"): # if the class is normal\n",
    "    print(\"The machine is normal\")\n",
    "    print()\n",
    "else: # if not then extract the top three fault types\n",
    "    fault_classes = similarity_measures[1:]\n",
    "    for i in range(3):\n",
    "        max_idx = np.argmax(fault_classes)\n",
    "        fault = label_mapping[max_idx + 1]\n",
    "        top3_faults.append(fault)\n",
    "        fault_classes[max_idx] = 0\n",
    "    print(\"Top most probable faults:\", top3_faults)\n",
    "    print()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "print(\"Time taken for prediction:\", (end_time - start_time)*1e3, \"msec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3226919e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1ba870",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7742dd9e",
   "metadata": {},
   "source": [
    "# RFC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "47961fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function just to give the estimation error\n",
    "def estimation_error(inc_vec, mem_mat):\n",
    "    # assuming memory matrix to be of the size L x M\n",
    "    L, M = mem_mat.shape\n",
    "    \n",
    "    # initialize a matrix G\n",
    "    G = np.zeros((L, L))\n",
    "    \n",
    "    # calculate elements of G using the similarity function\n",
    "    for i in range(L):\n",
    "        for j in range(L):\n",
    "            G[i, j] = calculate_similarity(mem_mat[i, :], mem_mat.T[:, j])\n",
    "    \n",
    "    # calculate inverse of the G matrix\n",
    "    inv_G = np.linalg.inv(G)\n",
    "    \n",
    "    # calculate the vector a_n\n",
    "    a_n = [calculate_similarity(mem_mat[i, :], inc_vec) for i in range(L)]\n",
    "    \n",
    "    # calculate the weight vector w_n\n",
    "    w_n = np.dot(inv_G, a_n)\n",
    "    \n",
    "    # calculate the normalized weight vector\n",
    "    normalized_w_n = w_n / np.linalg.norm(w_n, ord=1)\n",
    "    \n",
    "    # calculate the estimate\n",
    "    estimate = np.dot(mem_mat.T, normalized_w_n)\n",
    "    \n",
    "    # calculate the estimation error and its norm\n",
    "    estimate_error = inc_vec - estimate\n",
    "    estimate_error_norm = np.linalg.norm(estimate_error, ord=2)\n",
    "    \n",
    "    return estimate_error_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "df0f02e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# labeling each csv file\n",
    "\n",
    "# list of all the csv files\n",
    "files_list = ['normal_features.csv', 'imbalance_features.csv', 'horizontal_features.csv',\n",
    "              'vertical_features.csv', 'underhang_features.csv', 'overhang_features.csv']\n",
    "\n",
    "# list of the names of updated csv files\n",
    "updated_csv_names = ['normal_class_features.csv', 'imbalance_class_features.csv', 'horizontal_class_features.csv', \n",
    "                     'vertical_class_features.csv', 'underhang_class_features.csv', 'overhang_class_features.csv']\n",
    "\n",
    "for idx, file in enumerate(files_list):\n",
    "    df = pd.read_csv(file)\n",
    "    df[96] = idx\n",
    "    df.columns = list(range(97))\n",
    "    df.to_csv(updated_csv_names[idx],header=None,index=None)\n",
    "    \n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "dee87b5c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# list of the csv files\n",
    "csv_files = ['normal_class_features.csv', 'imbalance_class_features.csv', 'horizontal_class_features.csv', \n",
    "                     'vertical_class_features.csv', 'underhang_class_features.csv', 'overhang_class_features.csv']\n",
    "\n",
    "# split each csv file into training and testing sets\n",
    "for csv_file in csv_files:\n",
    "    df = pd.read_csv(csv_file, header=None)\n",
    "\n",
    "    # split the dataframe into features and labels\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "\n",
    "    # split into training (90%) and testing (10%) sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # save the training and testing sets to separate csv files\n",
    "    training_file = csv_file.replace('.csv', '_train.csv')\n",
    "    testing_file = csv_file.replace('.csv', '_test.csv')\n",
    "\n",
    "    pd.concat([X_train, y_train], axis=1).to_csv(training_file, index=False, header=False)\n",
    "    pd.concat([X_test, y_test], axis=1).to_csv(testing_file, index=False, header=False)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8b8dcce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# combine all training files into one training file\n",
    "training_files = [f.replace('.csv', '_train.csv') for f in csv_files]\n",
    "combined_training_file = 'combined_training_data.csv'\n",
    "\n",
    "combined_training_dfs = []\n",
    "for training_file in training_files:\n",
    "    df = pd.read_csv(training_file)\n",
    "    df.columns = list(range(97))\n",
    "    combined_training_dfs.append(df)\n",
    "\n",
    "# concatenate the data frames along rows\n",
    "combined_training_df = pd.concat(combined_training_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# save the combined training dataframe to a csv file\n",
    "combined_training_df.to_csv(combined_training_file, index=False, header=False)\n",
    "\n",
    "# combine all testing files into one testing file\n",
    "testing_files = [f.replace('.csv', '_test.csv') for f in csv_files]\n",
    "combined_testing_file = 'combined_testing_data.csv'\n",
    "\n",
    "combined_testing_dfs = []\n",
    "for testing_file in testing_files:\n",
    "    df = pd.read_csv(testing_file)\n",
    "    df.columns = list(range(97))\n",
    "    combined_testing_dfs.append(df)\n",
    "\n",
    "# concatenate the data frames along rows\n",
    "combined_testing_df = pd.concat(combined_testing_dfs, axis=0, ignore_index=True)\n",
    "\n",
    "# save the combined testing dataframe to a csv file\n",
    "combined_testing_df.to_csv(combined_testing_file, index=False, header=False)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "1e1d7679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "# also make a file of combined data\n",
    "combined_data_file = 'combined_data.csv'\n",
    "combined_dfs = []\n",
    "\n",
    "training_file_df = pd.read_csv('combined_training_data.csv')\n",
    "training_file_df.columns = list(range(97))\n",
    "combined_dfs.append(training_file_df)\n",
    "\n",
    "testing_file_df = pd.read_csv('combined_testing_data.csv')\n",
    "testing_file_df.columns = list(range(97))\n",
    "combined_dfs.append(testing_file_df)\n",
    "\n",
    "# concatenate the data frames along rows\n",
    "combined_df = pd.concat(combined_dfs, axis=0, ignore_index=True)\n",
    "combined_df = combined_df.sort_values(by=96)\n",
    "\n",
    "# save the combined testing dataframe to a csv file\n",
    "combined_df.to_csv(combined_data_file, index=False, header=False)\n",
    "\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "e959490c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Results:\n",
      "Accuracy: 0.9850249074296066\n",
      "Precision: 0.9857459873601092\n",
      "Recall: 0.9850249074296066\n",
      "F1-score: 0.9847950656554293\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import make_scorer, precision_score, recall_score, f1_score\n",
    "\n",
    "# load the combined data\n",
    "combined_data = pd.read_csv('combined_data.csv', header=None)\n",
    "\n",
    "X = combined_data.iloc[:, :-1]\n",
    "y = combined_data.iloc[:, -1]\n",
    "\n",
    "# initialize the Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# define the scoring metrics\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1': make_scorer(f1_score, average='weighted')\n",
    "}\n",
    "\n",
    "# Perform cross-validation with multiple scoring metrics\n",
    "cv_results = cross_validate(rf_classifier, X, y, scoring=scoring, cv=5)\n",
    "\n",
    "# Print the results\n",
    "print(\"Cross-Validation Results:\")\n",
    "print(\"Accuracy:\", cv_results['test_accuracy'].mean())\n",
    "print(\"Precision:\", cv_results['test_precision'].mean())\n",
    "print(\"Recall:\", cv_results['test_recall'].mean())\n",
    "print(\"F1-score:\", cv_results['test_f1'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "5dc6170f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[ 36   0   8   0   0   1]\n",
      " [  0 328   0   0   3   0]\n",
      " [  0   2 190   2   0   1]\n",
      " [  0   0   4 295   0   0]\n",
      " [  0   5   0   0 551   0]\n",
      " [  0   1   1   1   0 508]]\n"
     ]
    }
   ],
   "source": [
    "# confusion matrix for the above \n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred_cv = cross_val_predict(rf_classifier, X, y, cv=5)\n",
    "cm_cv = confusion_matrix(y, y_pred_cv)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06621c71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7fcd37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77c9300",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88acb068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd9fb99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
